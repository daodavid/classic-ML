# classic Machine Learning Algorithms
<hr> <hr>
<font size="4" face="Times New Roma" color="#3f134f">   
<h1> <a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html"> Linear Regression </a></h1>
  <h4>

    <ul style="margin-left: 30px">
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#simple~linear~regression">Simple Linear Regression </a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#grad~sim~linear">Gradient Descent over simple linear regression</a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#learning-rate">Effect of different values for learning rate</a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#m-linear-r">Multiple Linear Regression</a> </li> <br>
    <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#impl-multi">Implementation of gradient descent for Multiple Linear regression using NUMPY</a> </li> <br>
     <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#insurence">Test of our implemntation in 'insurance.csv' dataset </a> </li> <br>
     <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#MLE">The probabilistic approach to linear regression.Maximum likelihood estimation </a> </li> <br>
</ul> 

 </h4>
</font> 
<hr> <hr>

 <h1> <a href="https://daodavid.github.io/classic-ML/notes/reguralization.html">Regularization</a></h1>
 <font size="4" face="Times New Roma" color="#3f134f"> 
    <ul style="margin-left: 30px">
      <li><a href="https://daodavid.github.io/classic-ML/notes/reguralization.html#intro-pol"> Polynomial Regression, Bias and Variance </a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/reguralization.html#lasso"> Lasso Regression (L1 Regularization)</a> </li><br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/reguralization.html#feature"> Lasso as feature selection</a> </li><br>  
      <li><a href="https://daodavid.github.io/classic-ML/notes/reguralization.html#ridge"> Ridge regression (L2 regularization)</a> </li><br>          
      <li><a href="https://daodavid.github.io/classic-ML/notes/reguralization.html#k-fold">  K-fold cross validation </a> </li><br>       
      <li><a href="https://daodavid.github.io/classic-ML/notes/reguralization.html#ref"> References </a> </li><br>     
</ul>    
 </font>
<hr> <hr>
<h1> <a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html"> Logistic Regression </a></h1>
 <font size="4" face="Times New Roma" color="#3f134f"> 
 <ul style="margin-left: 30px">
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#odds-ration"> Log-odds or Loggit function  </a> </li> <br>
         <li><a href="#origin">The math origin of the Sigmoid function</a> </li><br>  
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#prop"> Properties and Identities Of Sigmoid Function</a> </li><br>  
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#max-li">Maximum Likelihood of Logistic regression, Cross-entropy Loss</a> </li><br>   
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#grad-descent">  Mathematical derivation of cross-entopy loss.Gradient Descent </a> </li><br>   
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#impl">   Implementation of BinaryLogisticRegression using numpy </a> </li><br>       
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#reg"> Reguralization of Logistic Regression  </a> </li><br>       
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#ref"> References </a> </li><br>     
</ul>    
 </font>
<hr> <hr>

<h1> <a href="https://daodavid.github.io/classic-ML/notes/softmax-regression.html">Soft-Max Regression</a></h1>
<font size="4" face="Times New Roma" color="#3f134f"> 
    <ul style="margin-left: 30px">
      <li><a href="https://daodavid.github.io/classic-ML/notes/softmax-regression.html#abstract">Abstract </a> </li> <br>
      <!--<li><a href='#int-1'>Introduction </a> </li><br> -->
      <li><a href="https://daodavid.github.io/classic-ML/notes/softmax-regression.html#deff_softmax">Softmaxt definition and  how it works?</a> </li><br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/softmax-regression.html#optimization">Optimizaton of  Softmax Loss with Gradient Descent (Deep math calculation)</a> </li><br>  
      <li><a href="https://daodavid.github.io/classic-ML/notes/softmax-regression.html#impl">Implementation of Softmax using numpy </a> </li><br>
       <li><a href="https://daodavid.github.io/classic-ML/notes/softmax-regression.html#reg">Regularization of softmax by learning rate and max iterations</a> </li><br> 
       <li><a href="https://daodavid.github.io/classic-ML/notes/softmax-regression.html#conclusion">Conclusion</a> </li><br>  

</ul>    
 </font>
<hr> <hr>

<h1> <a href="https://daodavid.github.io/classic-ML/notes/naive_bayes_classifier.html">Naive Bayes Classifier</a></h1>
<font size="4" face="Times New Roma" color="#3f134f"> 
    <ul style="margin-left: 30px">
      <li><a href="https://daodavid.github.io/classic-ML/notes/naive_bayes_classifier.html#bayes_theorem">Bayes Theorem</a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/naive_bayes_classifier.html#works">How does Binomial Naive Bayes work?</a> </li><br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/naive_bayes_classifier.html#likeli-invest">Investigation of likelihood and posterior probability  throw features values of Titanic dataset</a> </li><br>  
      <li><a href="https://daodavid.github.io/classic-ML/notes/naive_bayes_classifier.html#testing"> Implementation of likelihood table for Gaussian Naive Bayes and testing on Titanic </a> </li><br>
       <li><a href="https://daodavid.github.io/classic-ML/notes/naive_bayes_classifier.html#bernuli"> Bernoulli Naive Bayes</a> </li><br> 
       <li><a href="https://daodavid.github.io/classic-ML/notes/naive_bayes_classifier.html#ref">References</a> </li><br>  
    </ul>    
</font>
