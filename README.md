# classic Machine Learning Algorithms

  
<h1> <a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html"> Linear Regression </a></h1>
  <h4>
  <font size="4" face="Times New Roma" color="#3f134f"> 
    <ul style="margin-left: 30px">
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#simple~linear~regression">Simple Linear Regression </a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#grad~sim~linear">Gradient Descent over simple linear regression</a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#learning-rate">Effect of different values for learning rate</a> </li> <br>
      <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#m-linear-r">Multiple Linear Regression</a> </li> <br>
    <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#impl-multi">Implementation of gradient descent for Multiple Linear regression using NUMPY</a> </li> <br>
     <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#insurence">Test of our implemntation in 'insurance.csv' dataset </a> </li> <br>
     <li><a href="https://daodavid.github.io/classic-ML/notes/linear-regression.html#MLE">The probabilistic approach to linear regression.Maximum likelihood estimation </a> </li> <br>
</ul> 
</font>
 </h4>
 
 <hr>

<h1> <a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html"> Logistic Regression </a></h1>
 <font size="4" face="Times New Roma" color="#3f134f"> 
 <ul style="margin-left: 30px">
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#odds-ration"> Log-odds or Loggit function  </a> </li> <br>
         <li><a href="#origin">The math origin of the Sigmoid function</a> </li><br>  
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#prop"> Properties and Identities Of Sigmoid Function</a> </li><br>  
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#max-li">Maximum Likelihood of Logistic regression, Cross-entropy Loss</a> </li><br>   
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#grad-descent">  Mathematical derivation of cross-entopy loss.Gradient Descent </a> </li><br>   
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#impl">   Implementation of BinaryLogisticRegression using numpy </a> </li><br>       
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#reg"> Reguralization of Logistic Regression  </a> </li><br>       
      <li><a href="https://daodavid.github.io/classic-ML/notes/logistic_regression.html#ref"> References </a> </li><br>     
</ul>    
 </font>
