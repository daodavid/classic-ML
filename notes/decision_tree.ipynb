{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d73b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "from IPython.display import display, Markdown , Math \n",
    "\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35483a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string): display(Markdown(string))\n",
    "def latex(out): printmd(f'{out}')  \n",
    "def pr(string): printmd('***{}***'.format(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5eb4c",
   "metadata": {},
   "source": [
    "<h1> Deciosion Tree </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2adc25",
   "metadata": {},
   "source": [
    "<h2>\n",
    "  <p>\n",
    "    <a href =   \"https://github.com/daodavid\" > \n",
    "         author: daodeiv (David Stankov) \n",
    "       <img src=\"https://cdn.thenewstack.io/media/2014/12/github-octocat.png\" align=\"left\" width=\"120\"  alt=\"daodavid\" ></a>\n",
    "    </p>      \n",
    "</h2>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183536b6",
   "metadata": {},
   "source": [
    "  <h6>\n",
    "  <font size=\"4\" face = \"Times New Roma\" color='#3f134f' > \n",
    "    <ul style=\"margin-left: 30px\">\n",
    "      <li><a href='#bayes_theorem'>Bayes Theorem</a> </li> <br>\n",
    "      <li><a href='#works'>How does Binomial Naive Bayes work?</a> </li><br>\n",
    "      <li><a href='#likeli-invest'>Investigation of likelihood and posterior probability  throw features values of Titanic dataset</a> </li><br>  \n",
    "      <li><a href='#testing'> Implementation of likelihood table for Gaussian Naive Bayes and testing on Titanic </a> </li><br>\n",
    "       <li><a href='#bernuli'> Bernoulli Naive Bayes</a> </li><br> \n",
    "       <li><a href='#ref'>References</a> </li><br>  \n",
    "    </ul>    \n",
    "</font>\n",
    " </h6>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b401911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe586dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c5a66cf",
   "metadata": {},
   "source": [
    "For solving this attribute selection problem, researchers worked and devised some solutions. They suggested using some criteria like :\n",
    "\n",
    "*Entropy, <br>\n",
    "*Information gain, <br>\n",
    "*Gini index, <br>\n",
    "*Gain Ratio, <br>\n",
    "*Reduction in Variance <br>\n",
    "*Chi-Square <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce62a0",
   "metadata": {},
   "source": [
    "<h2 id='entropy'> Entropy (Entropy as expected surprise) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5afc7",
   "metadata": {},
   "source": [
    "The entropy was appeared in the thermodynamic field in the works of  Rudolf Clausius over the temperature of the gas. In ML the entropy plays an important row because it is a measure of chaos, disorder, or uncertainty of the system.\n",
    "The above graphics represent the two gasses initially they have a low entropy because they are cleanly separable, after some time, however, the gasses intermingle and the system's entropy increased. The entropy of a dataset is used to measure the impurity of a dataset and we will use this kind of informativeness measure in our calculations.\n",
    "<br> <br>\n",
    "\n",
    "\n",
    "First : If the probability of occurring of event A is $100%$  then your surprise $S$ should be zero.<br>\n",
    "if $P(A)=1%$ then $S(A)=0$ <br>\n",
    "Second : If something happens that you were totally sure was impossible, with 100% credence, then you should be infinitely surprised <br>\n",
    "\n",
    "\n",
    "That is, if E happens and $P(E) = 0$,then  $S = ∞$ <br> <br>\n",
    "\n",
    "$$S(0) = ∞ $$ <br>\n",
    "\n",
    "If an event $A_1$ happens that is surprising to degree $S_1$, and then another event $A_2$ happens with surprisingness $S_2$, then your surprise at the combination of these events should be $S1 + S2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ede49",
   "metadata": {},
   "source": [
    "I.e., we want surprise to be additive. If $$(P(A_1)) = S_1$$ and $$S(P(A_2 | A_1)) = S_2$$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7e160",
   "metadata": {},
   "source": [
    "then $$S(P(A_1 and A_2) = S_1 + S_2$$.\n",
    "\n",
    "This entails a new constraint on our surprise function, namely:\n",
    "\n",
    "$$S(PQ) = S(P) + S(Q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c5dcf",
   "metadata": {},
   "source": [
    "Fourth, and finally! We want our surprise function to be continuous – free from discontinuous jumps. If your credence that the event will happen changes by an arbitrarily small amount, then your surprise if it does happen should also change by an arbitrarily small amount.\n",
    "\n",
    "$$S(P)\\; is\\; continuous$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cfb490",
   "metadata": {},
   "source": [
    "Taking the simplest choice of k, we end up with a unique formalization of the intuitive notion of surprise:\n",
    "\n",
    "$$S(P) = – logP$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60aa4f",
   "metadata": {},
   "source": [
    "To summarize what we have so far: Four basic desideratum for our formalization of the intuitive notion of surprise have led us to a single simple equation.\n",
    "\n",
    "This equation that we’ve arrived at turns out to be extremely important in information theory. It is, in fact, just the definition of the amount of information you gain by observing E. This reveals to us a deep connection between surprise and information. They are in an important sense expressing the same basic idea: more surprising events give you more information, and unsurprising events give you little information.\n",
    "\n",
    "Let’s get a little better numerical sense of this formalization of surprise/information. What does a single unit of surprise or information mean? With some quick calculation, we see that a single unit of surprise, or bit of information corresponds to the observation of an event that you had a 50% expectation of. This also corresponds to a ruling out of 50% of the weight of possible other events you thought you might have observed. In essence, each bit of information you receive / surprise you experience corresponds to the total amount of possibilities being cut in half.\n",
    "\n",
    "Two bits of information narrow the possibilities to one-fourth. Three cut out all but one-eighth. And so on. For a rational agent, the process of receiving more information or of being continuously surprised is the process of whittling down your models of reality to a smaller and better set!\n",
    "\n",
    "The next great step forward is to use our formalization of surprise to talk not just about how surprised you are once an event happens, but how surprised you expect to be. If you have a credence of P in an event happening, then you expect a degree of surprise S(P) with credence P. In other words, the expected surprise you have with respect to that particular event is:\n",
    "\n",
    "$$Expected\\;\\;surprise = – p\\log p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1cea3",
   "metadata": {},
   "source": [
    "The next great step forward is to use our formalization of surprise to talk not just about how surprised you are once an event happens, but how surprised you expect to be. If you have a credence of P in an event happening, then you expect a degree of surprise S(P) with credence P. In other words, the expected surprise you have with respect to that particular event is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42aa943",
   "metadata": {},
   "source": [
    "When summed over the totality of all possible events that occurred we get the following expression:\n",
    "\n",
    "$Total\\;expected \\;surprise = – ∑_i P_i \\log P_i$\n",
    "\n",
    "This expression should look very very familiar to you. It’s one of the most important quantities humans have discovered…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a516d56",
   "metadata": {},
   "source": [
    "$$Entropy = Total\\; expected\\; surprise$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9150519",
   "metadata": {},
   "source": [
    "Total expected surprise is entropy. And entropy is a measure of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb42ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7e2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d83d8370",
   "metadata": {},
   "source": [
    "<h2 id='ref'> References </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ae247",
   "metadata": {},
   "source": [
    "*<a href='https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052'>Decision Trees in Machine Learning</a> <br>\n",
    "\n",
    "*<a href='https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html'>Decision Tree Algorithm, Explained</a>\n",
    "\n",
    "*<a href='https://risingentropy.com/entropy-is-total-expected-surprise/#:~:text=Total%20expected%20surprise%20is%20entropy,happen%20in%20the%20next%20moment'> Supricsd </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
